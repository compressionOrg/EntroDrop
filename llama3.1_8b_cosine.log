2025-07-22 21:04:55,731 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 21:05:03,961 - __main__ - INFO - Layers order: 29,30,28,27,23,18,15,26,17,21,20,19,25,16,24,22,14,11,8,12,4,5,1,3,6,9,0,7,2,10,13,31
2025-07-22 21:05:03,961 - __main__ - INFO - Num prune: 6
2025-07-22 21:05:03,962 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 18]
2025-07-22 21:05:03,962 - __main__ - INFO - ====================================================================================================
2025-07-22 21:05:03,964 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 21:05:57,229 - evaluate_grasp - INFO - wikitext2 156.3391571044922
2025-07-22 21:05:57,229 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 21:06:33,425 - evaluate_grasp - INFO - ptb 215.29820251464844
2025-07-22 23:43:08,978 - evaluate_grasp - INFO - {'wikitext2': 156.3391571044922, 'ptb': 215.29820251464844, 'mathqa': {'acc': 0.27738693467336684, 'acc_stderr': 0.008195897079410222, 'acc_norm': 0.2877721943048576, 'acc_norm_stderr': 0.00828770849477991}, 'piqa': {'acc': 0.6507072905331882, 'acc_stderr': 0.011123283817525078, 'acc_norm': 0.6512513601741022, 'acc_norm_stderr': 0.011119263056159592}, 'hellaswag': {'acc': 0.31627165903206533, 'acc_stderr': 0.004640699483543333, 'acc_norm': 0.3844851623182633, 'acc_norm_stderr': 0.004854791378656987}, 'winogrande': {'acc': np.float64(0.5966850828729282), 'acc_stderr': 0.013787257285896252}, 'arc_easy': {'acc': 0.4377104377104377, 'acc_stderr': 0.010179856486006906, 'acc_norm': 0.4318181818181818, 'acc_norm_stderr': 0.010163945352271721}, 'arc_challenge': {'acc': 0.29436860068259385, 'acc_stderr': 0.013318528460539427, 'acc_norm': 0.32081911262798635, 'acc_norm_stderr': 0.01364094309194653}, 'openbookqa': {'acc': 0.252, 'acc_stderr': 0.019435727282249512, 'acc_norm': 0.36, 'acc_norm_stderr': 0.021487751089720522}, 'boolq': {'acc': 0.6107033639143731, 'acc_stderr': 0.00852801629098454}, 'mean': np.float64(0.4294791711773691)}
2025-07-22 23:43:08,980 - evaluate_grasp - INFO - 

===== mean acc: 0.4294791711773691 =====


2025-07-22 23:43:13,465 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 23:43:26,279 - __main__ - INFO - Layers order: 29,30,28,27,23,18,15,26,17,21,20,19,25,16,24,22,14,11,8,12,4,5,1,3,6,9,0,7,2,10,13,31
2025-07-22 23:43:26,279 - __main__ - INFO - Num prune: 8
2025-07-22 23:43:26,280 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 18, 15, 26]
2025-07-22 23:43:26,280 - __main__ - INFO - ====================================================================================================
2025-07-22 23:43:26,282 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 23:44:42,088 - evaluate_grasp - INFO - wikitext2 342.4898986816406
2025-07-22 23:44:42,088 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 23:45:02,572 - evaluate_grasp - INFO - ptb 359.9535217285156
2025-07-23 01:35:51,114 - evaluate_grasp - INFO - {'wikitext2': 342.4898986816406, 'ptb': 359.9535217285156, 'mathqa': {'acc': 0.22814070351758794, 'acc_stderr': 0.007681942435552284, 'acc_norm': 0.23618090452261306, 'acc_norm_stderr': 0.00777531937874705}, 'piqa': {'acc': 0.6169749727965179, 'acc_stderr': 0.011342081709082852, 'acc_norm': 0.6120783460282916, 'acc_norm_stderr': 0.011368965300027388}, 'hellaswag': {'acc': 0.280920135431189, 'acc_stderr': 0.004485300194072271, 'acc_norm': 0.3234415455088628, 'acc_norm_stderr': 0.004668335725410293}, 'winogrande': {'acc': np.float64(0.5872138910812944), 'acc_stderr': 0.013837060648682087}, 'arc_easy': {'acc': 0.3686868686868687, 'acc_stderr': 0.009899640855681043, 'acc_norm': 0.37584175084175087, 'acc_norm_stderr': 0.009938436373170639}, 'arc_challenge': {'acc': 0.26535836177474403, 'acc_stderr': 0.012902554762313967, 'acc_norm': 0.2986348122866894, 'acc_norm_stderr': 0.013374078615068749}, 'openbookqa': {'acc': 0.242, 'acc_stderr': 0.019173085678337167, 'acc_norm': 0.34, 'acc_norm_stderr': 0.021206117013673066}, 'boolq': {'acc': 0.5724770642201835, 'acc_stderr': 0.008652692997177334}, 'mean': np.float64(0.39522149968854814)}
2025-07-23 01:35:51,116 - evaluate_grasp - INFO - 

===== mean acc: 0.39522149968854814 =====


2025-07-23 01:35:55,878 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-23 01:36:05,300 - __main__ - INFO - Layers order: 29,30,28,27,23,18,15,26,17,21,20,19,25,16,24,22,14,11,8,12,4,5,1,3,6,9,0,7,2,10,13,31
2025-07-23 01:36:05,300 - __main__ - INFO - Num prune: 10
2025-07-23 01:36:05,301 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 18, 15, 26, 17, 21]
2025-07-23 01:36:05,301 - __main__ - INFO - ====================================================================================================
2025-07-23 01:36:05,302 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-23 01:38:16,329 - evaluate_grasp - INFO - wikitext2 676.458740234375
2025-07-23 01:38:16,330 - evaluate_grasp - INFO - load dataset ptb
2025-07-23 01:38:56,860 - evaluate_grasp - INFO - ptb 749.0918579101562
2025-07-23 03:25:20,800 - evaluate_grasp - INFO - {'wikitext2': 676.458740234375, 'ptb': 749.0918579101562, 'mathqa': {'acc': 0.22177554438860972, 'acc_stderr': 0.007605186257370722, 'acc_norm': 0.23149078726968175, 'acc_norm_stderr': 0.007721327716271432}, 'piqa': {'acc': 0.5903155603917302, 'acc_stderr': 0.011473932007187606, 'acc_norm': 0.588683351468988, 'acc_norm_stderr': 0.011480860577192822}, 'hellaswag': {'acc': 0.28281218880701053, 'acc_stderr': 0.004494454911844627, 'acc_norm': 0.3110934076877116, 'acc_norm_stderr': 0.004619948037222914}, 'winogrande': {'acc': np.float64(0.5722178374112076), 'acc_stderr': 0.01390513401383994}, 'arc_easy': {'acc': 0.3653198653198653, 'acc_stderr': 0.009880576614806928, 'acc_norm': 0.3627946127946128, 'acc_norm_stderr': 0.009865936757013936}, 'arc_challenge': {'acc': 0.23976109215017063, 'acc_stderr': 0.012476304127453949, 'acc_norm': 0.29180887372013653, 'acc_norm_stderr': 0.013284525292403508}, 'openbookqa': {'acc': 0.2, 'acc_stderr': 0.01790645924143385, 'acc_norm': 0.316, 'acc_norm_stderr': 0.02081235951585586}, 'boolq': {'acc': 0.6244648318042814, 'acc_stderr': 0.008469774334938068}, 'mean': np.float64(0.38708336503410945)}
2025-07-23 03:25:20,802 - evaluate_grasp - INFO - 

===== mean acc: 0.38708336503410945 =====


2025-07-23 03:25:25,578 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-23 03:25:35,568 - __main__ - INFO - Layers order: 29,30,28,27,23,18,15,26,17,21,20,19,25,16,24,22,14,11,8,12,4,5,1,3,6,9,0,7,2,10,13,31
2025-07-23 03:25:35,568 - __main__ - INFO - Num prune: 12
2025-07-23 03:25:35,569 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 18, 15, 26, 17, 21, 20, 19]
2025-07-23 03:25:35,569 - __main__ - INFO - ====================================================================================================
2025-07-23 03:25:35,572 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-23 03:27:25,394 - evaluate_grasp - INFO - wikitext2 1019.8546142578125
2025-07-23 03:27:25,394 - evaluate_grasp - INFO - load dataset ptb
2025-07-23 03:27:57,484 - evaluate_grasp - INFO - ptb 1053.44580078125
2025-07-23 05:09:12,620 - evaluate_grasp - INFO - {'wikitext2': 1019.8546142578125, 'ptb': 1053.44580078125, 'mathqa': {'acc': 0.21909547738693466, 'acc_stderr': 0.0075720986970669035, 'acc_norm': 0.22579564489112228, 'acc_norm_stderr': 0.007653959786628141}, 'piqa': {'acc': 0.5810663764961915, 'acc_stderr': 0.01151147311999508, 'acc_norm': 0.5848748639825898, 'acc_norm_stderr': 0.01149652044265913}, 'hellaswag': {'acc': 0.29267078271260705, 'acc_stderr': 0.004540586983229985, 'acc_norm': 0.3497311292571201, 'acc_norm_stderr': 0.004759103432380767}, 'winogrande': {'acc': np.float64(0.5824782951854776), 'acc_stderr': 0.013859978264440248}, 'arc_easy': {'acc': 0.37457912457912457, 'acc_stderr': 0.009931758820410619, 'acc_norm': 0.3581649831649832, 'acc_norm_stderr': 0.009838331651451844}, 'arc_challenge': {'acc': 0.23976109215017063, 'acc_stderr': 0.012476304127453956, 'acc_norm': 0.2841296928327645, 'acc_norm_stderr': 0.013179442447653886}, 'openbookqa': {'acc': 0.164, 'acc_stderr': 0.016575811142446703, 'acc_norm': 0.294, 'acc_norm_stderr': 0.020395095484936603}, 'boolq': {'acc': 0.6480122324159021, 'acc_stderr': 0.008353104742682966}, 'mean': np.float64(0.38770792261580106)}
2025-07-23 05:09:12,621 - evaluate_grasp - INFO - 

===== mean acc: 0.38770792261580106 =====


2025-07-23 05:09:17,005 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-23 05:09:25,053 - __main__ - INFO - Layers order: 29,30,28,27,23,18,15,26,17,21,20,19,25,16,24,22,14,11,8,12,4,5,1,3,6,9,0,7,2,10,13,31
2025-07-23 05:09:25,053 - __main__ - INFO - Num prune: 14
2025-07-23 05:09:25,055 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 18, 15, 26, 17, 21, 20, 19, 25, 16]
2025-07-23 05:09:25,056 - __main__ - INFO - ====================================================================================================
2025-07-23 05:09:25,058 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-23 05:10:59,830 - evaluate_grasp - INFO - wikitext2 972065.0625
2025-07-23 05:10:59,830 - evaluate_grasp - INFO - load dataset ptb
2025-07-23 05:11:25,316 - evaluate_grasp - INFO - ptb 722126.75
2025-07-23 06:40:42,110 - evaluate_grasp - INFO - {'wikitext2': 972065.0625, 'ptb': 722126.75, 'mathqa': {'acc': 0.20737018425460638, 'acc_stderr': 0.0074217949215930334, 'acc_norm': 0.21105527638190955, 'acc_norm_stderr': 0.007470023801451696}, 'piqa': {'acc': 0.544069640914037, 'acc_stderr': 0.011620422647622234, 'acc_norm': 0.5375408052230686, 'acc_norm_stderr': 0.011632896120570523}, 'hellaswag': {'acc': 0.24895439155546703, 'acc_stderr': 0.004315236154543958, 'acc_norm': 0.23302131049591715, 'acc_norm_stderr': 0.004218917037002668}, 'winogrande': {'acc': np.float64(0.5556432517758485), 'acc_stderr': 0.013965196769083555}, 'arc_easy': {'acc': 0.3135521885521885, 'acc_stderr': 0.009519779157242258, 'acc_norm': 0.3021885521885522, 'acc_norm_stderr': 0.00942271904248319}, 'arc_challenge': {'acc': 0.2354948805460751, 'acc_stderr': 0.01239945185500475, 'acc_norm': 0.2790102389078498, 'acc_norm_stderr': 0.013106784883601352}, 'openbookqa': {'acc': 0.168, 'acc_stderr': 0.016736553541541903, 'acc_norm': 0.29, 'acc_norm_stderr': 0.020313179231745186}, 'boolq': {'acc': 0.6269113149847095, 'acc_stderr': 0.008458661252058382}, 'mean': np.float64(0.36249948157286654)}
2025-07-23 06:40:42,112 - evaluate_grasp - INFO - 

===== mean acc: 0.36249948157286654 =====


2025-07-23 06:40:46,353 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-23 06:40:55,884 - __main__ - INFO - Layers order: 29,30,28,27,23,18,15,26,17,21,20,19,25,16,24,22,14,11,8,12,4,5,1,3,6,9,0,7,2,10,13,31
2025-07-23 06:40:55,884 - __main__ - INFO - Num prune: 16
2025-07-23 06:40:55,885 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 18, 15, 26, 17, 21, 20, 19, 25, 16, 24, 22]
2025-07-23 06:40:55,885 - __main__ - INFO - ====================================================================================================
2025-07-23 06:40:55,886 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-23 06:42:16,537 - evaluate_grasp - INFO - wikitext2 2691.13134765625
2025-07-23 06:42:16,538 - evaluate_grasp - INFO - load dataset ptb
2025-07-23 06:42:39,693 - evaluate_grasp - INFO - ptb 2811.833984375
2025-07-23 08:08:42,303 - evaluate_grasp - INFO - {'wikitext2': 2691.13134765625, 'ptb': 2811.833984375, 'mathqa': {'acc': 0.2067001675041876, 'acc_stderr': 0.007412926365358813, 'acc_norm': 0.20770519262981574, 'acc_norm_stderr': 0.007426217631188543}, 'piqa': {'acc': 0.559847660500544, 'acc_stderr': 0.011581954727227393, 'acc_norm': 0.545157780195865, 'acc_norm_stderr': 0.011618148261187403}, 'hellaswag': {'acc': 0.280920135431189, 'acc_stderr': 0.004485300194072271, 'acc_norm': 0.32503485361481776, 'acc_norm_stderr': 0.004674306182532151}, 'winogrande': {'acc': np.float64(0.5477505919494869), 'acc_stderr': 0.01398825621660602}, 'arc_easy': {'acc': 0.3148148148148148, 'acc_stderr': 0.009530150430975593, 'acc_norm': 0.30892255892255893, 'acc_norm_stderr': 0.009481048387761351}, 'arc_challenge': {'acc': 0.22184300341296928, 'acc_stderr': 0.012141659068147886, 'acc_norm': 0.27474402730375425, 'acc_norm_stderr': 0.013044617212771227}, 'openbookqa': {'acc': 0.144, 'acc_stderr': 0.015716934945725767, 'acc_norm': 0.292, 'acc_norm_stderr': 0.020354375480530085}, 'boolq': {'acc': 0.6351681957186545, 'acc_stderr': 0.008419440984963635}, 'mean': np.float64(0.36388057116648076)}
2025-07-23 08:08:42,305 - evaluate_grasp - INFO - 

===== mean acc: 0.36388057116648076 =====


