2025-07-21 16:27:35,524 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-21 16:27:44,666 - __main__ - INFO - Layers order: 29,30,28,27,23,17,13,25,14,21,20,19,22,16,24,26,15,7,6,12,4,5,2,3,8,11,1,9,0,10,18,31
2025-07-21 16:27:44,666 - __main__ - INFO - Num prune: 4
2025-07-21 16:27:44,667 - __main__ - INFO - Layers to remove: [29, 30, 28, 27]
2025-07-21 16:27:44,667 - __main__ - INFO - ====================================================================================================
2025-07-21 16:27:44,669 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-21 16:32:36,790 - evaluate_grasp - INFO - wikitext2 137.94395446777344
2025-07-21 16:32:36,790 - evaluate_grasp - INFO - load dataset ptb
2025-07-21 16:34:03,704 - evaluate_grasp - INFO - ptb 189.48817443847656
2025-07-21 16:40:55,958 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B




2025-07-21 19:58:37,279 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-21 19:58:45,098 - __main__ - INFO - Layers order: 29,30,28,27,23,17,13,25,14,21,20,19,22,16,24,26,15,7,6,12,4,5,2,3,8,11,1,9,0,10,18,31
2025-07-21 19:58:45,098 - __main__ - INFO - Num prune: 8
2025-07-21 19:58:45,100 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 17, 13, 25]
2025-07-21 19:58:45,100 - __main__ - INFO - ====================================================================================================
2025-07-21 19:58:45,103 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-21 20:03:22,854 - evaluate_grasp - INFO - wikitext2 343.6874694824219
2025-07-21 20:03:22,854 - evaluate_grasp - INFO - load dataset ptb
2025-07-21 20:04:46,008 - evaluate_grasp - INFO - ptb 477.837646484375
2025-07-21 20:54:11,816 - evaluate_grasp - INFO - {'wikitext2': 859.9703369140625, 'ptb': 724.8302612304688, 'mathqa': {'acc': 0.240536013400335, 'acc_stderr': 0.007824277362109035, 'acc_norm': 0.2455611390284757, 'acc_norm_stderr': 0.007879387071710741}, 'piqa': {'acc': 0.5908596300326442, 'acc_stderr': 0.011471593460443316, 'acc_norm': 0.5952121871599565, 'acc_norm_stderr': 0.011452361375057023}, 'hellaswag': {'acc': 0.279326827325234, 'acc_stderr': 0.004477514681328155, 'acc_norm': 0.3080063732324238, 'acc_norm_stderr': 0.004607256752931884}, 'winogrande': {'acc': np.float64(0.5580110497237569), 'acc_stderr': 0.013957584079109001}, 'arc_easy': {'acc': 0.3442760942760943, 'acc_stderr': 0.009749495321590813, 'acc_norm': 0.3371212121212121, 'acc_norm_stderr': 0.009700146509130076}, 'arc_challenge': {'acc': 0.24658703071672355, 'acc_stderr': 0.01259572626879012, 'acc_norm': 0.2858361774744027, 'acc_norm_stderr': 0.013203196088537369}, 'openbookqa': {'acc': 0.222, 'acc_stderr': 0.01860441475825008, 'acc_norm': 0.328, 'acc_norm_stderr': 0.021017027165175492}, 'boolq': {'acc': 0.6207951070336392, 'acc_stderr': 0.008486012137246294}, 'mean': np.float64(0.3877989690635534)}
2025-07-21 20:54:11,817 - evaluate_grasp - INFO - 

===== mean acc: 0.3877989690635534 =====




2025-07-21 23:13:02,968 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-21 23:13:12,612 - __main__ - INFO - Layers order: 29,30,28,27,23,17,13,25,14,21,20,19,22,16,24,26,15,7,6,12,4,5,2,3,8,11,1,9,0,10,18,31
2025-07-21 23:13:12,612 - __main__ - INFO - Num prune: 12
2025-07-21 23:13:12,614 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 17, 13, 25, 14, 21, 20, 19]
2025-07-21 23:13:12,614 - __main__ - INFO - ====================================================================================================
2025-07-21 23:13:12,617 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-21 23:15:03,730 - evaluate_grasp - INFO - {'wikitext2': 4794.1376953125, 'ptb': 3466.98974609375, 'mathqa': {'acc': 0.20904522613065327, 'acc_stderr': 0.007443831666570556, 'acc_norm': 0.20033500837520937, 'acc_norm_stderr': 0.007327115822178691}, 'piqa': {'acc': 0.5554951033732318, 'acc_stderr': 0.011593746871584154, 'acc_norm': 0.5331882480957563, 'acc_norm_stderr': 0.011640096923563138}, 'hellaswag': {'acc': 0.27185819557857, 'acc_stderr': 0.004440079173276965, 'acc_norm': 0.2884883489344752, 'acc_norm_stderr': 0.004521334761709215}, 'winogrande': {'acc': np.float64(0.4980268350434096), 'acc_stderr': 0.014052376259225627}, 'arc_easy': {'acc': 0.33291245791245794, 'acc_stderr': 0.009669958978395328, 'acc_norm': 0.3148148148148148, 'acc_norm_stderr': 0.009530150430975616}, 'arc_challenge': {'acc': 0.2440273037542662, 'acc_stderr': 0.012551447627856257, 'acc_norm': 0.295221843003413, 'acc_norm_stderr': 0.01332975029338232}, 'openbookqa': {'acc': 0.19, 'acc_stderr': 0.017561800410759033, 'acc_norm': 0.268, 'acc_norm_stderr': 0.019827714859587574}, 'boolq': {'acc': 0.6168195718654435, 'acc_stderr': 0.00850302139145079}, 'mean': np.float64(0.364773086707254)}
2025-07-21 23:15:03,732 - evaluate_grasp - INFO - 

===== mean acc: 0.364773086707254 =====


2025-07-21 23:17:17,982 - evaluate_grasp - INFO - wikitext2 1474.046875
2025-07-21 23:17:17,982 - evaluate_grasp - INFO - load dataset ptb
2025-07-21 23:18:24,766 - evaluate_grasp - INFO - ptb 1565.8953857421875
2025-07-22 01:40:53,029 - evaluate_grasp - INFO - {'wikitext2': 1474.046875, 'ptb': 1565.8953857421875, 'mathqa': {'acc': 0.22110552763819097, 'acc_stderr': 0.007596957582219331, 'acc_norm': 0.22311557788944725, 'acc_norm_stderr': 0.007621557738201545}, 'piqa': {'acc': 0.5957562568008705, 'acc_stderr': 0.01144989176300747, 'acc_norm': 0.5788900979325353, 'acc_norm_stderr': 0.011519701059151491}, 'hellaswag': {'acc': 0.2884883489344752, 'acc_stderr': 0.004521334761709213, 'acc_norm': 0.3333001394144593, 'acc_norm_stderr': 0.004704293898729905}, 'winogrande': {'acc': np.float64(0.5643251775848461), 'acc_stderr': 0.013935709739615712}, 'arc_easy': {'acc': 0.3463804713804714, 'acc_stderr': 0.009763542075695734, 'acc_norm': 0.3501683501683502, 'acc_norm_stderr': 0.009788295410093148}, 'arc_challenge': {'acc': 0.23976109215017063, 'acc_stderr': 0.01247630412745395, 'acc_norm': 0.29692832764505117, 'acc_norm_stderr': 0.013352025976725223}, 'openbookqa': {'acc': 0.176, 'acc_stderr': 0.01704785202062227, 'acc_norm': 0.31, 'acc_norm_stderr': 0.020704041021724802}, 'boolq': {'acc': 0.6229357798165137, 'acc_stderr': 0.00847660292795373}, 'mean': np.float64(0.38184408178819235)}
2025-07-22 01:40:53,031 - evaluate_grasp - INFO - 

===== mean acc: 0.38184408178819235 =====


2025-07-22 01:40:57,396 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 01:41:05,389 - __main__ - INFO - Layers order: 29,30,28,27,23,17,13,25,14,21,20,19,22,16,24,26,15,7,6,12,4,5,2,3,8,11,1,9,0,10,18,31
2025-07-22 01:41:05,390 - __main__ - INFO - Num prune: 16
2025-07-22 01:41:05,391 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 17, 13, 25, 14, 21, 20, 19, 22, 16, 24, 26]
2025-07-22 01:41:05,391 - __main__ - INFO - ====================================================================================================
2025-07-22 01:41:05,393 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 01:44:02,459 - evaluate_grasp - INFO - wikitext2 4826.7880859375
2025-07-22 01:44:02,459 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 01:44:56,258 - evaluate_grasp - INFO - ptb 2985.67529296875
2025-07-22 02:58:50,819 - evaluate_grasp - INFO - {'wikitext2': 4826.7880859375, 'ptb': 2985.67529296875, 'mathqa': {'acc': 0.20804020100502513, 'acc_stderr': 0.007430632646805398, 'acc_norm': 0.2100502512562814, 'acc_norm_stderr': 0.007456961930598774}, 'piqa': {'acc': 0.5478781284004353, 'acc_stderr': 0.011612217507379625, 'acc_norm': 0.515233949945593, 'acc_norm_stderr': 0.011660408257153634}, 'hellaswag': {'acc': 0.2719577773351922, 'acc_stderr': 0.004440588618232715, 'acc_norm': 0.2927703644692292, 'acc_norm_stderr': 0.004541039698729836}, 'winogrande': {'acc': np.float64(0.5343330702446725), 'acc_stderr': 0.014019317531542558}, 'arc_easy': {'acc': 0.31734006734006737, 'acc_stderr': 0.00955064834394777, 'acc_norm': 0.30808080808080807, 'acc_norm_stderr': 0.009473887075826332}, 'arc_challenge': {'acc': 0.23890784982935154, 'acc_stderr': 0.012461071376316621, 'acc_norm': 0.2721843003412969, 'acc_norm_stderr': 0.013006600406423707}, 'openbookqa': {'acc': 0.174, 'acc_stderr': 0.016971271257516147, 'acc_norm': 0.278, 'acc_norm_stderr': 0.02005583388807091}, 'boolq': {'acc': 0.6305810397553517, 'acc_stderr': 0.008441557531799617}, 'mean': np.float64(0.36537976673876194)}
2025-07-22 02:58:50,821 - evaluate_grasp - INFO - 

===== mean acc: 0.36537976673876194 =====


2025-07-22 11:02:48,858 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 11:02:56,407 - __main__ - INFO - Layers order: 29,30,28,27,23,17,13,25,14,21,20,19,22,16,24,26,15,7,6,12,4,5,2,3,8,11,1,9,0,10,18,31
2025-07-22 11:02:56,407 - __main__ - INFO - Num prune: 10
2025-07-22 11:02:56,408 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 17, 13, 25, 14, 21]
2025-07-22 11:02:56,408 - __main__ - INFO - ====================================================================================================
2025-07-22 11:02:56,410 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 11:03:34,745 - evaluate_grasp - INFO - wikitext2 788.9345703125
2025-07-22 11:03:34,745 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 11:03:46,396 - evaluate_grasp - INFO - ptb 902.5696411132812
2025-07-22 11:04:37,799 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 11:04:45,557 - __main__ - INFO - Layers order: 29,30,28,27,23,17,13,25,14,21,20,19,22,16,24,26,15,7,6,12,4,5,2,3,8,11,1,9,0,10,18,31
2025-07-22 11:04:45,558 - __main__ - INFO - Num prune: 6
2025-07-22 11:04:45,558 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 17]
2025-07-22 11:04:45,558 - __main__ - INFO - ====================================================================================================
2025-07-22 11:04:45,561 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 11:05:32,742 - evaluate_grasp - INFO - wikitext2 168.3256072998047
2025-07-22 11:05:32,743 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 11:05:54,128 - evaluate_grasp - INFO - ptb 225.3159942626953
2025-07-22 13:32:07,322 - evaluate_grasp - INFO - {'wikitext2': 168.3256072998047, 'ptb': 225.3159942626953, 'mathqa': {'acc': 0.29447236180904524, 'acc_stderr': 0.008344107220961077, 'acc_norm': 0.3018425460636516, 'acc_norm_stderr': 0.008403641322847266}, 'piqa': {'acc': 0.6273122959738846, 'acc_stderr': 0.011281318332897744, 'acc_norm': 0.6322089227421109, 'acc_norm_stderr': 0.011250616646678792}, 'hellaswag': {'acc': 0.303326030671181, 'acc_stderr': 0.004587553577101238, 'acc_norm': 0.35859390559649473, 'acc_norm_stderr': 0.004786075107572178}, 'winogrande': {'acc': np.float64(0.6022099447513812), 'acc_stderr': 0.013755743513749025}, 'arc_easy': {'acc': 0.40614478114478114, 'acc_stderr': 0.010077409815364063, 'acc_norm': 0.4036195286195286, 'acc_norm_stderr': 0.01006736896034821}, 'arc_challenge': {'acc': 0.2960750853242321, 'acc_stderr': 0.013340916085246263, 'acc_norm': 0.3242320819112628, 'acc_norm_stderr': 0.01367881039951882}, 'openbookqa': {'acc': 0.248, 'acc_stderr': 0.019332342821239103, 'acc_norm': 0.344, 'acc_norm_stderr': 0.02126575803797874}, 'boolq': {'acc': 0.6159021406727829, 'acc_stderr': 0.008506861063860251}, 'mean': np.float64(0.42418033004341105)}
2025-07-22 13:32:07,324 - evaluate_grasp - INFO - 

===== mean acc: 0.42418033004341105 =====


2025-07-22 13:32:11,842 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 13:32:20,300 - __main__ - INFO - Layers order: 29,30,28,27,23,17,13,25,14,21,20,19,22,16,24,26,15,7,6,12,4,5,2,3,8,11,1,9,0,10,18,31
2025-07-22 13:32:20,300 - __main__ - INFO - Num prune: 10
2025-07-22 13:32:20,301 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 17, 13, 25, 14, 21]
2025-07-22 13:32:20,301 - __main__ - INFO - ====================================================================================================
2025-07-22 13:32:20,302 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 13:33:53,827 - evaluate_grasp - INFO - wikitext2 788.9345703125
2025-07-22 13:33:53,827 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 13:34:24,289 - evaluate_grasp - INFO - ptb 902.5696411132812
2025-07-22 14:51:51,444 - evaluate_grasp - INFO - {'wikitext2': 788.9345703125, 'ptb': 902.5696411132812, 'mathqa': {'acc': 0.2304857621440536, 'acc_stderr': 0.007709584482517432, 'acc_norm': 0.23785594639865998, 'acc_norm_stderr': 0.007794282274854806}, 'piqa': {'acc': 0.5946681175190425, 'acc_stderr': 0.011454816387346774, 'acc_norm': 0.5892274211099021, 'acc_norm_stderr': 0.01147856555677578}, 'hellaswag': {'acc': 0.2757418840868353, 'acc_stderr': 0.004459740315490876, 'acc_norm': 0.30561641107349136, 'acc_norm_stderr': 0.004597265399568737}, 'winogrande': {'acc': np.float64(0.5501183898973955), 'acc_stderr': 0.01398171190404973}, 'arc_easy': {'acc': 0.3303872053872054, 'acc_stderr': 0.00965143021642818, 'acc_norm': 0.31734006734006737, 'acc_norm_stderr': 0.009550648343947768}, 'arc_challenge': {'acc': 0.23720136518771331, 'acc_stderr': 0.01243039982926084, 'acc_norm': 0.2645051194539249, 'acc_norm_stderr': 0.012889272949313366}, 'openbookqa': {'acc': 0.19, 'acc_stderr': 0.017561800410758995, 'acc_norm': 0.304, 'acc_norm_stderr': 0.020591649571224925}, 'boolq': {'acc': 0.6217125382262997, 'acc_stderr': 0.008482001133931005}, 'mean': np.float64(0.3787894078060681)}
2025-07-22 14:51:51,445 - evaluate_grasp - INFO - 

===== mean acc: 0.3787894078060681 =====


2025-07-22 14:51:55,441 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 14:52:04,152 - __main__ - INFO - Layers order: 29,30,28,27,23,17,13,25,14,21,20,19,22,16,24,26,15,7,6,12,4,5,2,3,8,11,1,9,0,10,18,31
2025-07-22 14:52:04,152 - __main__ - INFO - Num prune: 14
2025-07-22 14:52:04,153 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 23, 17, 13, 25, 14, 21, 20, 19, 22, 16]
2025-07-22 14:52:04,153 - __main__ - INFO - ====================================================================================================
2025-07-22 14:52:04,156 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 14:53:29,387 - evaluate_grasp - INFO - wikitext2 9016.130859375
2025-07-22 14:53:29,388 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 14:53:51,641 - evaluate_grasp - INFO - ptb 5283.57958984375
2025-07-22 16:01:20,104 - evaluate_grasp - INFO - {'wikitext2': 9016.130859375, 'ptb': 5283.57958984375, 'mathqa': {'acc': 0.21306532663316582, 'acc_stderr': 0.007495943791881955, 'acc_norm': 0.21708542713567838, 'acc_norm_stderr': 0.007546978526071608}, 'piqa': {'acc': 0.5554951033732318, 'acc_stderr': 0.011593746871584154, 'acc_norm': 0.5337323177366703, 'acc_norm_stderr': 0.011639245522413971}, 'hellaswag': {'acc': 0.2758414658434575, 'acc_stderr': 0.004460238879247425, 'acc_norm': 0.30611431985660226, 'acc_norm_stderr': 0.004599358920909542}, 'winogrande': {'acc': np.float64(0.5461720599842147), 'acc_stderr': 0.01399244156370707}, 'arc_easy': {'acc': 0.3223905723905724, 'acc_stderr': 0.009590672908157436, 'acc_norm': 0.3143939393939394, 'acc_norm_stderr': 0.009526702423162905}, 'arc_challenge': {'acc': 0.22866894197952217, 'acc_stderr': 0.012272853582540795, 'acc_norm': 0.2858361774744027, 'acc_norm_stderr': 0.013203196088537365}, 'openbookqa': {'acc': 0.164, 'acc_stderr': 0.01657581114244669, 'acc_norm': 0.308, 'acc_norm_stderr': 0.020667032987466104}, 'boolq': {'acc': 0.6012232415902141, 'acc_stderr': 0.008563973987729909}, 'mean': np.float64(0.3633570889742973)}
2025-07-22 16:01:20,105 - evaluate_grasp - INFO - 

===== mean acc: 0.3633570889742973 =====


