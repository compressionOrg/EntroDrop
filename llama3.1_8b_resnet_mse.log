
2025-07-21 16:44:55,829 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-21 16:45:05,761 - __main__ - INFO - Layers order: 29,30,28,27,24,16,14,25,13,21,20,19,23,18,22,26,15,9,7,12,4,6,1,3,5,11,0,8,2,10,17,31
2025-07-21 16:45:05,761 - __main__ - INFO - Num prune: 6
2025-07-21 16:45:05,761 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 24, 16]
2025-07-21 16:45:05,761 - __main__ - INFO - ====================================================================================================
2025-07-21 16:45:05,763 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-21 16:46:00,059 - evaluate_grasp - INFO - wikitext2 239.07655334472656
2025-07-21 16:46:00,060 - evaluate_grasp - INFO - load dataset ptb
2025-07-21 16:46:13,842 - evaluate_grasp - INFO - ptb 270.2705078125
2025-07-21 18:11:43,273 - evaluate_grasp - INFO - {'wikitext2': 239.07655334472656, 'ptb': 270.2705078125, 'mathqa': {'acc': 0.29547738693467335, 'acc_stderr': 0.008352378831993178, 'acc_norm': 0.30217755443886096, 'acc_norm_stderr': 0.00840628594817176}, 'piqa': {'acc': 0.6294885745375408, 'acc_stderr': 0.011267826475447665, 'acc_norm': 0.6376496191512514, 'acc_norm_stderr': 0.011215040215104572}, 'hellaswag': {'acc': 0.3028281218880701, 'acc_stderr': 0.004585424513012097, 'acc_norm': 0.3602867954590719, 'acc_norm_stderr': 0.004791024004588015}, 'winogrande': {'acc': np.float64(0.6029992107340174), 'acc_stderr': 0.013751092519806702}, 'arc_easy': {'acc': 0.42803030303030304, 'acc_stderr': 0.010152943316426272, 'acc_norm': 0.4116161616161616, 'acc_norm_stderr': 0.01009821864671491}, 'arc_challenge': {'acc': 0.2909556313993174, 'acc_stderr': 0.013273077865907588, 'acc_norm': 0.31143344709897613, 'acc_norm_stderr': 0.01353247209985094}, 'openbookqa': {'acc': 0.276, 'acc_stderr': 0.020011219298073542, 'acc_norm': 0.386, 'acc_norm_stderr': 0.021793529219281165}, 'boolq': {'acc': 0.4847094801223242, 'acc_stderr': 0.00874096483222434}, 'mean': np.float64(0.4138110885807808)}
2025-07-21 18:11:43,275 - evaluate_grasp - INFO - 

===== mean acc: 0.4138110885807808 =====

2025-07-21 18:11:47,248 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-21 18:11:55,679 - __main__ - INFO - Layers order: 29,30,28,27,24,16,14,25,13,21,20,19,23,18,22,26,15,9,7,12,4,6,1,3,5,11,0,8,2,10,17,31
2025-07-21 18:11:55,679 - __main__ - INFO - Num prune: 8
2025-07-21 18:11:55,681 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 24, 16, 14, 25]
2025-07-21 18:11:55,681 - __main__ - INFO - ====================================================================================================
2025-07-21 18:11:55,684 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-21 18:13:40,972 - evaluate_grasp - INFO - wikitext2 392.7971496582031
2025-07-21 18:13:40,972 - evaluate_grasp - INFO - load dataset ptb
2025-07-21 18:14:11,412 - evaluate_grasp - INFO - ptb 423.6163330078125
2025-07-21 19:34:08,192 - evaluate_grasp - INFO - {'wikitext2': 392.7971496582031, 'ptb': 423.6163330078125, 'mathqa': {'acc': 0.2613065326633166, 'acc_stderr': 0.008042810817625763, 'acc_norm': 0.2606365159128978, 'acc_norm_stderr': 0.008036134931668057}, 'piqa': {'acc': 0.6120783460282916, 'acc_stderr': 0.011368965300027381, 'acc_norm': 0.6142546245919478, 'acc_norm_stderr': 0.011357166777524042}, 'hellaswag': {'acc': 0.28599880501892055, 'acc_stderr': 0.004509652679395692, 'acc_norm': 0.3291177056363274, 'acc_norm_stderr': 0.004689324696186876}, 'winogrande': {'acc': np.float64(0.5651144435674822), 'acc_stderr': 0.013932814110418029}, 'arc_easy': {'acc': 0.38173400673400676, 'acc_stderr': 0.00996864885183967, 'acc_norm': 0.38215488215488214, 'acc_norm_stderr': 0.009970747281292448}, 'arc_challenge': {'acc': 0.25, 'acc_stderr': 0.012653835621466646, 'acc_norm': 0.28924914675767915, 'acc_norm_stderr': 0.013250012579393443}, 'openbookqa': {'acc': 0.24, 'acc_stderr': 0.01911886665375974, 'acc_norm': 0.336, 'acc_norm_stderr': 0.021144791425048843}, 'boolq': {'acc': 0.6220183486238532, 'acc_stderr': 0.008480656964585245}, 'mean': np.float64(0.4022813103294839)}
2025-07-21 19:34:08,193 - evaluate_grasp - INFO - 

===== mean acc: 0.4022813103294839 =====

2025-07-21 19:34:12,337 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-21 19:34:20,134 - __main__ - INFO - Layers order: 29,30,28,27,24,16,14,25,13,21,20,19,23,18,22,26,15,9,7,12,4,6,1,3,5,11,0,8,2,10,17,31
2025-07-21 19:34:20,134 - __main__ - INFO - Num prune: 10
2025-07-21 19:34:20,136 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 24, 16, 14, 25, 13, 21]
2025-07-21 19:34:20,136 - __main__ - INFO - ====================================================================================================
2025-07-21 19:34:20,140 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-21 19:36:07,734 - evaluate_grasp - INFO - wikitext2 859.9703369140625
2025-07-21 19:36:07,734 - evaluate_grasp - INFO - load dataset ptb
2025-07-21 19:36:37,253 - evaluate_grasp - INFO - ptb 724.8302612304688
2025-07-21 19:58:32,943 - evaluate_grasp - INFO - {'wikitext2': 137.94395446777344, 'ptb': 189.48817443847656, 'mathqa': {'acc': 0.33668341708542715, 'acc_stderr': 0.008651110935825696, 'acc_norm': 0.34472361809045227, 'acc_norm_stderr': 0.008700583267963934}, 'piqa': {'acc': 0.6343852013057671, 'acc_stderr': 0.011236571679006276, 'acc_norm': 0.6343852013057671, 'acc_norm_stderr': 0.011236571679006276}, 'hellaswag': {'acc': 0.3088030272854013, 'acc_stderr': 0.00461055497441124, 'acc_norm': 0.367257518422625, 'acc_norm_stderr': 0.004810723108378219}, 'winogrande': {'acc': np.float64(0.5919494869771112), 'acc_stderr': 0.013812822643745028}, 'arc_easy': {'acc': 0.43223905723905726, 'acc_stderr': 0.010165130379698753, 'acc_norm': 0.4318181818181818, 'acc_norm_stderr': 0.010163945352271725}, 'arc_challenge': {'acc': 0.310580204778157, 'acc_stderr': 0.013522292098053059, 'acc_norm': 0.33532423208191126, 'acc_norm_stderr': 0.013796182947785566}, 'openbookqa': {'acc': 0.274, 'acc_stderr': 0.019966103540279452, 'acc_norm': 0.392, 'acc_norm_stderr': 0.02185468495561126}, 'boolq': {'acc': 0.6039755351681957, 'acc_stderr': 0.008553881336813417}, 'mean': np.float64(0.4365769912298896)}
2025-07-21 19:58:32,945 - evaluate_grasp - INFO - 

===== mean acc: 0.4365769912298896 =====

2025-07-21 20:54:15,961 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-21 20:54:25,033 - __main__ - INFO - Layers order: 29,30,28,27,24,16,14,25,13,21,20,19,23,18,22,26,15,9,7,12,4,6,1,3,5,11,0,8,2,10,17,31
2025-07-21 20:54:25,033 - __main__ - INFO - Num prune: 12
2025-07-21 20:54:25,034 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 24, 16, 14, 25, 13, 21, 20, 19]
2025-07-21 20:54:25,034 - __main__ - INFO - ====================================================================================================
2025-07-21 20:54:25,036 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-21 20:55:59,827 - evaluate_grasp - INFO - wikitext2 9500.53515625
2025-07-21 20:55:59,827 - evaluate_grasp - INFO - load dataset ptb
2025-07-21 20:56:26,785 - evaluate_grasp - INFO - ptb 2842.86328125
2025-07-21 22:09:45,258 - evaluate_grasp - INFO - {'wikitext2': 9500.53515625, 'ptb': 2842.86328125, 'mathqa': {'acc': 0.2338358458961474, 'acc_stderr': 0.0077484894980075075, 'acc_norm': 0.23618090452261306, 'acc_norm_stderr': 0.007775319378747051}, 'piqa': {'acc': 0.5919477693144722, 'acc_stderr': 0.011466872778651264, 'acc_norm': 0.5794341675734495, 'acc_norm_stderr': 0.011517665611282774}, 'hellaswag': {'acc': 0.2927703644692292, 'acc_stderr': 0.004541039698729829, 'acc_norm': 0.34773949412467636, 'acc_norm_stderr': 0.004752794829825042}, 'winogrande': {'acc': np.float64(0.5540647198105761), 'acc_stderr': 0.013970093482330699}, 'arc_easy': {'acc': 0.36447811447811446, 'acc_stderr': 0.009875729282482438, 'acc_norm': 0.359006734006734, 'acc_norm_stderr': 0.009843424713072176}, 'arc_challenge': {'acc': 0.24232081911262798, 'acc_stderr': 0.012521593295800116, 'acc_norm': 0.2781569965870307, 'acc_norm_stderr': 0.0130944699195388}, 'openbookqa': {'acc': 0.204, 'acc_stderr': 0.01803936910413867, 'acc_norm': 0.328, 'acc_norm_stderr': 0.021017027165175495}, 'boolq': {'acc': 0.6244648318042814, 'acc_stderr': 0.008469774334938068}, 'mean': np.float64(0.38848530811068105)}
2025-07-21 22:09:45,259 - evaluate_grasp - INFO - 

===== mean acc: 0.38848530811068105 =====

2025-07-21 22:09:49,502 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-21 22:09:58,680 - __main__ - INFO - Layers order: 29,30,28,27,24,16,14,25,13,21,20,19,23,18,22,26,15,9,7,12,4,6,1,3,5,11,0,8,2,10,17,31
2025-07-21 22:09:58,680 - __main__ - INFO - Num prune: 16
2025-07-21 22:09:58,681 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 24, 16, 14, 25, 13, 21, 20, 19, 23, 18, 22, 26]
2025-07-21 22:09:58,681 - __main__ - INFO - ====================================================================================================
2025-07-21 22:09:58,682 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-21 22:11:13,109 - evaluate_grasp - INFO - wikitext2 4794.1376953125
2025-07-21 22:11:13,109 - evaluate_grasp - INFO - load dataset ptb
2025-07-21 22:11:35,288 - evaluate_grasp - INFO - ptb 3466.98974609375
2025-07-21 23:12:58,499 - evaluate_grasp - INFO - {'wikitext2': 343.6874694824219, 'ptb': 477.837646484375, 'mathqa': {'acc': 0.2629815745393635, 'acc_stderr': 0.008059394672720424, 'acc_norm': 0.26666666666666666, 'acc_norm_stderr': 0.008095350740048933}, 'piqa': {'acc': 0.6109902067464635, 'acc_stderr': 0.011374774974447463, 'acc_norm': 0.5990206746463548, 'acc_norm_stderr': 0.011434766962108309}, 'hellaswag': {'acc': 0.27743477394941246, 'acc_stderr': 0.004468178273665663, 'acc_norm': 0.30920135431189005, 'acc_norm_stderr': 0.004612198061600089}, 'winogrande': {'acc': np.float64(0.5864246250986582), 'acc_stderr': 0.013840971763195306}, 'arc_easy': {'acc': 0.34385521885521886, 'acc_stderr': 0.00974666058485245, 'acc_norm': 0.3404882154882155, 'acc_norm_stderr': 0.00972367681382587}, 'arc_challenge': {'acc': 0.2593856655290102, 'acc_stderr': 0.012808273573927104, 'acc_norm': 0.28924914675767915, 'acc_norm_stderr': 0.013250012579393443}, 'openbookqa': {'acc': 0.22, 'acc_stderr': 0.01854421137582033, 'acc_norm': 0.326, 'acc_norm_stderr': 0.02098400956239357}, 'boolq': {'acc': 0.6217125382262997, 'acc_stderr': 0.008482001133931005}, 'mean': np.float64(0.39784807536805333)}
2025-07-21 23:12:58,501 - evaluate_grasp - INFO - 

===== mean acc: 0.39784807536805333 =====2025-07-22 11:05:38,897 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 11:05:47,434 - __main__ - INFO - Layers order: 29,30,28,27,24,16,14,25,13,21,20,19,23,18,22,26,15,9,7,12,4,6,1,3,5,11,0,8,2,10,17,31
2025-07-22 11:05:47,435 - __main__ - INFO - Num prune: 14
2025-07-22 11:05:47,437 - __main__ - INFO - Layers to remove: [29, 30, 28, 27, 24, 16, 14, 25, 13, 21, 20, 19, 23, 18]
2025-07-22 11:05:47,437 - __main__ - INFO - ====================================================================================================
2025-07-22 11:05:47,441 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 11:06:53,813 - evaluate_grasp - INFO - wikitext2 139435.3125
2025-07-22 11:06:53,813 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 11:07:14,841 - evaluate_grasp - INFO - ptb 27570462.0
2025-07-22 13:14:30,538 - evaluate_grasp - INFO - {'wikitext2': 139435.3125, 'ptb': 27570462.0, 'mathqa': {'acc': 0.21708542713567838, 'acc_stderr': 0.007546978526071593, 'acc_norm': 0.20569514237855946, 'acc_norm_stderr': 0.007399565480241924}, 'piqa': {'acc': 0.5750816104461371, 'acc_stderr': 0.011533547946654765, 'acc_norm': 0.5549510337323177, 'acc_norm_stderr': 0.011595157509775765}, 'hellaswag': {'acc': 0.28818960366460866, 'acc_stderr': 0.00451994171650834, 'acc_norm': 0.33608842859988053, 'acc_norm_stderr': 0.0047140416525986144}, 'winogrande': {'acc': np.float64(0.5572217837411207), 'acc_stderr': 0.013960157350784996}, 'arc_easy': {'acc': 0.3627946127946128, 'acc_stderr': 0.009865936757013935, 'acc_norm': 0.33585858585858586, 'acc_norm_stderr': 0.009691180932083494}, 'arc_challenge': {'acc': 0.257679180887372, 'acc_stderr': 0.012780770562768409, 'acc_norm': 0.28924914675767915, 'acc_norm_stderr': 0.013250012579393443}, 'openbookqa': {'acc': 0.188, 'acc_stderr': 0.017490678880346215, 'acc_norm': 0.306, 'acc_norm_stderr': 0.020629569998345393}, 'boolq': {'acc': 0.6229357798165137, 'acc_stderr': 0.00847660292795373}, 'mean': np.float64(0.38362349981075544)}
2025-07-22 13:14:30,540 - evaluate_grasp - INFO - 

===== mean acc: 0.38362349981075544 =====


