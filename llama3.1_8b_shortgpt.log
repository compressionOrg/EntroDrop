2025-07-22 10:36:22,031 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 10:36:31,649 - __main__ - INFO - Layers order: 25,26,24,27,23,28,22,29,20,21,19,18,30,17,16,11,10,13,15,14,9,12,8,7,3,6,4,2,5,1,31,0
2025-07-22 10:36:31,650 - __main__ - INFO - Num prune: 4
2025-07-22 10:36:31,650 - __main__ - INFO - Layers to remove: [25, 26, 24, 27]
2025-07-22 10:36:31,650 - __main__ - INFO - ====================================================================================================
2025-07-22 10:36:31,652 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 10:37:42,598 - evaluate_grasp - INFO - wikitext2 14.20435619354248
2025-07-22 10:37:42,598 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 10:38:20,906 - evaluate_grasp - INFO - ptb 20.490264892578125
2025-07-22 13:58:47,488 - evaluate_grasp - INFO - {'wikitext2': 14.20435619354248, 'ptb': 20.490264892578125, 'mathqa': {'acc': 0.325963149078727, 'acc_stderr': 0.008580777946942422, 'acc_norm': 0.32830820770519265, 'acc_norm_stderr': 0.008596595288591137}, 'piqa': {'acc': 0.7573449401523396, 'acc_stderr': 0.010002002569708698, 'acc_norm': 0.7763873775843307, 'acc_norm_stderr': 0.009721489519176283}, 'hellaswag': {'acc': 0.5503883688508265, 'acc_stderr': 0.004964378762425246, 'acc_norm': 0.7379008165704043, 'acc_norm_stderr': 0.004388775298210197}, 'winogrande': {'acc': np.float64(0.7032359905288083), 'acc_stderr': 0.012839239695202022}, 'arc_easy': {'acc': 0.7222222222222222, 'acc_stderr': 0.009190779909649923, 'acc_norm': 0.7129629629629629, 'acc_norm_stderr': 0.009282621598983085}, 'arc_challenge': {'acc': 0.44197952218430037, 'acc_stderr': 0.014512682523128345, 'acc_norm': 0.47525597269624575, 'acc_norm_stderr': 0.014593487694937742}, 'openbookqa': {'acc': 0.312, 'acc_stderr': 0.020740596536488076, 'acc_norm': 0.416, 'acc_norm_stderr': 0.02206494331392887}, 'boolq': {'acc': 0.7055045871559633, 'acc_stderr': 0.007972265028476174}, 'mean': np.float64(0.5648298475216484)}
2025-07-22 13:58:47,489 - evaluate_grasp - INFO - 

===== mean acc: 0.5648298475216484 =====


2025-07-22 13:58:51,772 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 13:59:01,642 - __main__ - INFO - Layers order: 25,26,24,27,23,28,22,29,20,21,19,18,30,17,16,11,10,13,15,14,9,12,8,7,3,6,4,2,5,1,31,0
2025-07-22 13:59:01,643 - __main__ - INFO - Num prune: 6
2025-07-22 13:59:01,644 - __main__ - INFO - Layers to remove: [25, 26, 24, 27, 23, 28]
2025-07-22 13:59:01,644 - __main__ - INFO - ====================================================================================================
2025-07-22 13:59:01,646 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 14:03:38,150 - evaluate_grasp - INFO - wikitext2 45.94759750366211
2025-07-22 14:03:38,150 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 14:04:58,100 - evaluate_grasp - INFO - ptb 45.26976776123047
2025-07-22 17:23:48,944 - evaluate_grasp - INFO - {'wikitext2': 45.94759750366211, 'ptb': 45.26976776123047, 'mathqa': {'acc': 0.30083752093802346, 'acc_stderr': 0.008395675569924642, 'acc_norm': 0.29413735343383585, 'acc_norm_stderr': 0.008341339176593602}, 'piqa': {'acc': 0.7252448313384113, 'acc_stderr': 0.010415033676676042, 'acc_norm': 0.7426550598476604, 'acc_norm_stderr': 0.01019992106479251}, 'hellaswag': {'acc': 0.4998008364867556, 'acc_stderr': 0.004989781015595466, 'acc_norm': 0.6823341963752241, 'acc_norm_stderr': 0.004646172373101}, 'winogrande': {'acc': np.float64(0.6850828729281768), 'acc_stderr': 0.013054277568469221}, 'arc_easy': {'acc': 0.6393097643097643, 'acc_stderr': 0.00985351210841673, 'acc_norm': 0.6233164983164983, 'acc_norm_stderr': 0.009942848077476165}, 'arc_challenge': {'acc': 0.39590443686006827, 'acc_stderr': 0.014291228393536585, 'acc_norm': 0.4377133105802048, 'acc_norm_stderr': 0.014497573881108288}, 'openbookqa': {'acc': 0.254, 'acc_stderr': 0.01948659680164338, 'acc_norm': 0.374, 'acc_norm_stderr': 0.02166071034720448}, 'boolq': {'acc': 0.7198776758409786, 'acc_stderr': 0.007854087822506234}, 'mean': np.float64(0.5275072423377724)}
2025-07-22 17:23:48,946 - evaluate_grasp - INFO - 

===== mean acc: 0.5275072423377724 =====


2025-07-22 17:23:53,189 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 17:24:01,224 - __main__ - INFO - Layers order: 25,26,24,27,23,28,22,29,20,21,19,18,30,17,16,11,10,13,15,14,9,12,8,7,3,6,4,2,5,1,31,0
2025-07-22 17:24:01,224 - __main__ - INFO - Num prune: 8
2025-07-22 17:24:01,226 - __main__ - INFO - Layers to remove: [25, 26, 24, 27, 23, 28, 22, 29]
2025-07-22 17:24:01,226 - __main__ - INFO - ====================================================================================================
2025-07-22 17:24:01,231 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 17:28:32,792 - evaluate_grasp - INFO - wikitext2 2811.855224609375
2025-07-22 17:28:32,792 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 17:29:56,785 - evaluate_grasp - INFO - ptb 2822.57666015625
2025-07-22 20:37:18,585 - evaluate_grasp - INFO - {'wikitext2': 2811.855224609375, 'ptb': 2822.57666015625, 'mathqa': {'acc': 0.2576214405360134, 'acc_stderr': 0.00800579201170253, 'acc_norm': 0.25695142378559466, 'acc_norm_stderr': 0.007998981788423124}, 'piqa': {'acc': 0.6050054406964092, 'acc_stderr': 0.011405665187969023, 'acc_norm': 0.6126224156692056, 'acc_norm_stderr': 0.011366038083435911}, 'hellaswag': {'acc': 0.28211511651065524, 'acc_stderr': 0.004491093528113428, 'acc_norm': 0.30860386377215693, 'acc_norm_stderr': 0.004609731925736901}, 'winogrande': {'acc': np.float64(0.5430149960536701), 'acc_stderr': 0.01400038676159829}, 'arc_easy': {'acc': 0.42845117845117847, 'acc_stderr': 0.010154195733990975, 'acc_norm': 0.4145622895622896, 'acc_norm_stderr': 0.010108889212447772}, 'arc_challenge': {'acc': 0.27303754266211605, 'acc_stderr': 0.013019332762635734, 'acc_norm': 0.3199658703071672, 'acc_norm_stderr': 0.013631345807016198}, 'openbookqa': {'acc': 0.18, 'acc_stderr': 0.01719859247631427, 'acc_norm': 0.29, 'acc_norm_stderr': 0.020313179231745197}, 'boolq': {'acc': 0.37553516819571864, 'acc_stderr': 0.008469774334938068}, 'mean': np.float64(0.3680976103882202)}
2025-07-22 20:37:18,587 - evaluate_grasp - INFO - 

===== mean acc: 0.3680976103882202 =====


2025-07-22 20:37:22,890 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 20:37:32,186 - __main__ - INFO - Layers order: 25,26,24,27,23,28,22,29,20,21,19,18,30,17,16,11,10,13,15,14,9,12,8,7,3,6,4,2,5,1,31,0
2025-07-22 20:37:32,186 - __main__ - INFO - Num prune: 10
2025-07-22 20:37:32,187 - __main__ - INFO - Layers to remove: [25, 26, 24, 27, 23, 28, 22, 29, 20, 21]
2025-07-22 20:37:32,187 - __main__ - INFO - ====================================================================================================
2025-07-22 20:37:32,190 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 20:41:33,697 - evaluate_grasp - INFO - wikitext2 14202.0556640625
2025-07-22 20:41:33,697 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 20:42:45,734 - evaluate_grasp - INFO - ptb 22515.287109375
2025-07-22 23:23:05,418 - evaluate_grasp - INFO - {'wikitext2': 14202.0556640625, 'ptb': 22515.287109375, 'mathqa': {'acc': 0.2422110552763819, 'acc_stderr': 0.007842810183504984, 'acc_norm': 0.24623115577889448, 'acc_norm_stderr': 0.007886624866001852}, 'piqa': {'acc': 0.6093579978237215, 'acc_stderr': 0.011383377760053584, 'acc_norm': 0.6158868335146899, 'acc_norm_stderr': 0.011348160741479135}, 'hellaswag': {'acc': 0.2764389563831906, 'acc_stderr': 0.004463224445470979, 'acc_norm': 0.3116908982274447, 'acc_norm_stderr': 0.004622376674166723}, 'winogrande': {'acc': np.float64(0.5761641673243884), 'acc_stderr': 0.013888492389944515}, 'arc_easy': {'acc': 0.4414983164983165, 'acc_stderr': 0.010189314382749939, 'acc_norm': 0.42424242424242425, 'acc_norm_stderr': 0.010141333654958569}, 'arc_challenge': {'acc': 0.28754266211604096, 'acc_stderr': 0.01322671905626613, 'acc_norm': 0.3174061433447099, 'acc_norm_stderr': 0.01360223908803817}, 'openbookqa': {'acc': 0.182, 'acc_stderr': 0.01727277329773045, 'acc_norm': 0.306, 'acc_norm_stderr': 0.0206295699983454}, 'boolq': {'acc': 0.5217125382262997, 'acc_stderr': 0.008736805647519946}, 'mean': np.float64(0.3921157117060424)}
2025-07-22 23:23:05,420 - evaluate_grasp - INFO - 

===== mean acc: 0.3921157117060424 =====


2025-07-22 23:23:10,185 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 23:23:20,312 - __main__ - INFO - Layers order: 25,26,24,27,23,28,22,29,20,21,19,18,30,17,16,11,10,13,15,14,9,12,8,7,3,6,4,2,5,1,31,0
2025-07-22 23:23:20,313 - __main__ - INFO - Num prune: 12
2025-07-22 23:23:20,314 - __main__ - INFO - Layers to remove: [25, 26, 24, 27, 23, 28, 22, 29, 20, 21, 19, 18]
2025-07-22 23:23:20,314 - __main__ - INFO - ====================================================================================================
2025-07-22 23:23:20,315 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 23:25:20,462 - evaluate_grasp - INFO - wikitext2 80382.125
2025-07-22 23:25:20,462 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 23:25:55,091 - evaluate_grasp - INFO - ptb 126376.8515625
2025-07-23 01:06:54,211 - evaluate_grasp - INFO - {'wikitext2': 80382.125, 'ptb': 126376.8515625, 'mathqa': {'acc': 0.22278056951423786, 'acc_stderr': 0.007617475572803638, 'acc_norm': 0.21943048576214405, 'acc_norm_stderr': 0.007576259919649274}, 'piqa': {'acc': 0.6011969532100109, 'acc_stderr': 0.01142439054503728, 'acc_norm': 0.588683351468988, 'acc_norm_stderr': 0.011480860577192817}, 'hellaswag': {'acc': 0.3062139016132245, 'acc_stderr': 0.004599776866717471, 'acc_norm': 0.35062736506671976, 'acc_norm_stderr': 0.004761912511707506}, 'winogrande': {'acc': np.float64(0.5509076558800315), 'acc_stderr': 0.013979459389140851}, 'arc_easy': {'acc': 0.40025252525252525, 'acc_stderr': 0.010053550119896127, 'acc_norm': 0.39225589225589225, 'acc_norm_stderr': 0.010018744689650042}, 'arc_challenge': {'acc': 0.27559726962457337, 'acc_stderr': 0.013057169655761838, 'acc_norm': 0.30631399317406144, 'acc_norm_stderr': 0.013470584417276513}, 'openbookqa': {'acc': 0.178, 'acc_stderr': 0.017123622189062257, 'acc_norm': 0.298, 'acc_norm_stderr': 0.02047511809298897}, 'boolq': {'acc': 0.5596330275229358, 'acc_stderr': 0.0086826356676869}, 'mean': np.float64(0.38682273782719234)}
2025-07-23 01:06:54,213 - evaluate_grasp - INFO - 

===== mean acc: 0.38682273782719234 =====


2025-07-23 01:06:58,999 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-23 01:07:15,004 - __main__ - INFO - Layers order: 25,26,24,27,23,28,22,29,20,21,19,18,30,17,16,11,10,13,15,14,9,12,8,7,3,6,4,2,5,1,31,0
2025-07-23 01:07:15,004 - __main__ - INFO - Num prune: 14
2025-07-23 01:07:15,006 - __main__ - INFO - Layers to remove: [25, 26, 24, 27, 23, 28, 22, 29, 20, 21, 19, 18, 30, 17]
2025-07-23 01:07:15,006 - __main__ - INFO - ====================================================================================================
2025-07-23 01:07:15,007 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-23 01:09:06,092 - evaluate_grasp - INFO - wikitext2 1052.607666015625
2025-07-23 01:09:06,093 - evaluate_grasp - INFO - load dataset ptb
2025-07-23 01:09:39,129 - evaluate_grasp - INFO - ptb 1190.826904296875
2025-07-23 02:42:12,529 - evaluate_grasp - INFO - {'wikitext2': 1052.607666015625, 'ptb': 1190.826904296875, 'mathqa': {'acc': 0.20569514237855946, 'acc_stderr': 0.0073995654802419255, 'acc_norm': 0.2134003350083752, 'acc_norm_stderr': 0.007500237530346826}, 'piqa': {'acc': 0.5712731229597389, 'acc_stderr': 0.011546694435712199, 'acc_norm': 0.5701849836779108, 'acc_norm_stderr': 0.011550322268694077}, 'hellaswag': {'acc': 0.29426409081856203, 'acc_stderr': 0.004547798964126683, 'acc_norm': 0.3428599880501892, 'acc_norm_stderr': 0.004736950810617789}, 'winogrande': {'acc': np.float64(0.5730071033938438), 'acc_stderr': 0.013901878072575057}, 'arc_easy': {'acc': 0.3611111111111111, 'acc_stderr': 0.00985601342581124, 'acc_norm': 0.359006734006734, 'acc_norm_stderr': 0.009843424713072174}, 'arc_challenge': {'acc': 0.2551194539249147, 'acc_stderr': 0.012739038695202102, 'acc_norm': 0.2909556313993174, 'acc_norm_stderr': 0.013273077865907583}, 'openbookqa': {'acc': 0.176, 'acc_stderr': 0.017047852020622242, 'acc_norm': 0.294, 'acc_norm_stderr': 0.02039509548493661}, 'boolq': {'acc': 0.6238532110091743, 'acc_stderr': 0.008472516562330725}, 'mean': np.float64(0.382540404449488)}
2025-07-23 02:42:12,531 - evaluate_grasp - INFO - 

===== mean acc: 0.382540404449488 =====


2025-07-23 02:42:17,171 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-23 02:42:26,536 - __main__ - INFO - Layers order: 25,26,24,27,23,28,22,29,20,21,19,18,30,17,16,11,10,13,15,14,9,12,8,7,3,6,4,2,5,1,31,0
2025-07-23 02:42:26,536 - __main__ - INFO - Num prune: 16
2025-07-23 02:42:26,538 - __main__ - INFO - Layers to remove: [25, 26, 24, 27, 23, 28, 22, 29, 20, 21, 19, 18, 30, 17, 16, 11]
2025-07-23 02:42:26,538 - __main__ - INFO - ====================================================================================================
2025-07-23 02:42:26,539 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-23 02:44:04,527 - evaluate_grasp - INFO - wikitext2 2162.924072265625
2025-07-23 02:44:04,527 - evaluate_grasp - INFO - load dataset ptb
2025-07-23 02:44:33,319 - evaluate_grasp - INFO - ptb 1902.7589111328125
2025-07-23 04:08:36,680 - evaluate_grasp - INFO - {'wikitext2': 2162.924072265625, 'ptb': 1902.7589111328125, 'mathqa': {'acc': 0.2241206030150754, 'acc_stderr': 0.007633761575437862, 'acc_norm': 0.21474036850921274, 'acc_norm_stderr': 0.0075173379276184124}, 'piqa': {'acc': 0.5636561479869423, 'acc_stderr': 0.011570895640553715, 'acc_norm': 0.5353645266594124, 'acc_norm_stderr': 0.011636607860111553}, 'hellaswag': {'acc': 0.27305317665803625, 'acc_stderr': 0.004446173999993624, 'acc_norm': 0.3035251941844254, 'acc_norm_stderr': 0.004588403419449677}, 'winogrande': {'acc': np.float64(0.5406471981057617), 'acc_stderr': 0.014005973823825138}, 'arc_easy': {'acc': 0.33585858585858586, 'acc_stderr': 0.009691180932083513, 'acc_norm': 0.3207070707070707, 'acc_norm_stderr': 0.00957747457110883}, 'arc_challenge': {'acc': 0.24061433447098976, 'acc_stderr': 0.012491468532390564, 'acc_norm': 0.28924914675767915, 'acc_norm_stderr': 0.013250012579393443}, 'openbookqa': {'acc': 0.16, 'acc_stderr': 0.0164115409805023, 'acc_norm': 0.31, 'acc_norm_stderr': 0.020704041021724795}, 'boolq': {'acc': 0.6244648318042814, 'acc_stderr': 0.008469774334938068}, 'mean': np.float64(0.3703018597374591)}
2025-07-23 04:08:36,682 - evaluate_grasp - INFO - 

===== mean acc: 0.3703018597374591 =====


