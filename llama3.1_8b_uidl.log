2025-07-22 10:51:32,589 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 10:51:40,497 - __main__ - INFO - Layers order: 24,25,26,27,28,29,30,31,21,20,19,22,18,17,16,15,8,14,10,7,9,11,13,6,12,5,4,3,2,23,1,0
2025-07-22 10:51:40,497 - __main__ - INFO - Num prune: 4
2025-07-22 10:51:40,498 - __main__ - INFO - Layers to remove: [24, 25, 26, 27]
2025-07-22 10:51:40,498 - __main__ - INFO - ====================================================================================================
2025-07-22 10:51:40,502 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 10:56:11,391 - evaluate_grasp - INFO - wikitext2 14.20435619354248
2025-07-22 10:56:11,392 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 10:57:28,071 - evaluate_grasp - INFO - ptb 20.490264892578125
2025-07-22 14:36:13,615 - evaluate_grasp - INFO - {'wikitext2': 14.20435619354248, 'ptb': 20.490264892578125, 'mathqa': {'acc': 0.325963149078727, 'acc_stderr': 0.008580777946942422, 'acc_norm': 0.32830820770519265, 'acc_norm_stderr': 0.008596595288591137}, 'piqa': {'acc': 0.7573449401523396, 'acc_stderr': 0.010002002569708698, 'acc_norm': 0.7763873775843307, 'acc_norm_stderr': 0.009721489519176283}, 'hellaswag': {'acc': 0.5503883688508265, 'acc_stderr': 0.004964378762425246, 'acc_norm': 0.7379008165704043, 'acc_norm_stderr': 0.004388775298210197}, 'winogrande': {'acc': np.float64(0.7032359905288083), 'acc_stderr': 0.012839239695202022}, 'arc_easy': {'acc': 0.7222222222222222, 'acc_stderr': 0.009190779909649923, 'acc_norm': 0.7129629629629629, 'acc_norm_stderr': 0.009282621598983085}, 'arc_challenge': {'acc': 0.44197952218430037, 'acc_stderr': 0.014512682523128345, 'acc_norm': 0.47525597269624575, 'acc_norm_stderr': 0.014593487694937742}, 'openbookqa': {'acc': 0.312, 'acc_stderr': 0.020740596536488076, 'acc_norm': 0.416, 'acc_norm_stderr': 0.02206494331392887}, 'boolq': {'acc': 0.7055045871559633, 'acc_stderr': 0.007972265028476174}, 'mean': np.float64(0.5648298475216484)}
2025-07-22 14:36:13,617 - evaluate_grasp - INFO - 

===== mean acc: 0.5648298475216484 =====


2025-07-22 14:36:17,995 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 14:36:27,061 - __main__ - INFO - Layers order: 24,25,26,27,28,29,30,31,21,20,19,22,18,17,16,15,8,14,10,7,9,11,13,6,12,5,4,3,2,23,1,0
2025-07-22 14:36:27,061 - __main__ - INFO - Num prune: 6
2025-07-22 14:36:27,062 - __main__ - INFO - Layers to remove: [24, 25, 26, 27, 28, 29]
2025-07-22 14:36:27,062 - __main__ - INFO - ====================================================================================================
2025-07-22 14:36:27,064 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 14:40:56,981 - evaluate_grasp - INFO - wikitext2 992.8682861328125
2025-07-22 14:40:56,981 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 14:42:16,479 - evaluate_grasp - INFO - ptb 1705.7806396484375
2025-07-22 17:59:28,492 - evaluate_grasp - INFO - {'wikitext2': 992.8682861328125, 'ptb': 1705.7806396484375, 'mathqa': {'acc': 0.3085427135678392, 'acc_stderr': 0.008455531595847356, 'acc_norm': 0.3001675041876047, 'acc_norm_stderr': 0.008390338453387805}, 'piqa': {'acc': 0.5984766050054406, 'acc_stderr': 0.011437324373397844, 'acc_norm': 0.6158868335146899, 'acc_norm_stderr': 0.011348160741479143}, 'hellaswag': {'acc': 0.2773351921927903, 'acc_stderr': 0.0044676841327724115, 'acc_norm': 0.3060147380999801, 'acc_norm_stderr': 0.0045989407223740895}, 'winogrande': {'acc': np.float64(0.5832675611681136), 'acc_stderr': 0.013856250072796316}, 'arc_easy': {'acc': 0.44907407407407407, 'acc_stderr': 0.010206428316323363, 'acc_norm': 0.44991582491582494, 'acc_norm_stderr': 0.010208181969301794}, 'arc_challenge': {'acc': 0.2781569965870307, 'acc_stderr': 0.013094469919538797, 'acc_norm': 0.3267918088737201, 'acc_norm_stderr': 0.013706665975587342}, 'openbookqa': {'acc': 0.232, 'acc_stderr': 0.018896193591952062, 'acc_norm': 0.308, 'acc_norm_stderr': 0.0206670329874661}, 'boolq': {'acc': 0.37889908256880733, 'acc_stderr': 0.008484678718565017}, 'mean': np.float64(0.388219028145512)}
2025-07-22 17:59:28,493 - evaluate_grasp - INFO - 

===== mean acc: 0.388219028145512 =====


2025-07-22 17:59:32,799 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 17:59:42,487 - __main__ - INFO - Layers order: 24,25,26,27,28,29,30,31,21,20,19,22,18,17,16,15,8,14,10,7,9,11,13,6,12,5,4,3,2,23,1,0
2025-07-22 17:59:42,488 - __main__ - INFO - Num prune: 8
2025-07-22 17:59:42,489 - __main__ - INFO - Layers to remove: [24, 25, 26, 27, 28, 29, 30, 31]
2025-07-22 17:59:42,489 - __main__ - INFO - ====================================================================================================
2025-07-22 17:59:42,493 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 18:03:47,109 - evaluate_grasp - INFO - wikitext2 1583.4376220703125
2025-07-22 18:03:47,109 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 18:04:58,962 - evaluate_grasp - INFO - ptb 2014.3929443359375
2025-07-22 21:10:57,002 - evaluate_grasp - INFO - {'wikitext2': 1583.4376220703125, 'ptb': 2014.3929443359375, 'mathqa': {'acc': 0.32931323283082076, 'acc_stderr': 0.008603299672361434, 'acc_norm': 0.32864321608040203, 'acc_norm_stderr': 0.008598835037935642}, 'piqa': {'acc': 0.6996735582154516, 'acc_stderr': 0.010695225308183138, 'acc_norm': 0.6855277475516867, 'acc_norm_stderr': 0.010833009065106567}, 'hellaswag': {'acc': 0.40131447918741286, 'acc_stderr': 0.004891626718097262, 'acc_norm': 0.5257916749651463, 'acc_norm_stderr': 0.00498313847960438}, 'winogrande': {'acc': np.float64(0.6085240726124704), 'acc_stderr': 0.013717487071290854}, 'arc_easy': {'acc': 0.6094276094276094, 'acc_stderr': 0.010011059112064256, 'acc_norm': 0.601010101010101, 'acc_norm_stderr': 0.010048240683798748}, 'arc_challenge': {'acc': 0.37457337883959047, 'acc_stderr': 0.01414419347189344, 'acc_norm': 0.41638225255972694, 'acc_norm_stderr': 0.014405618279436176}, 'openbookqa': {'acc': 0.306, 'acc_stderr': 0.020629569998345407, 'acc_norm': 0.396, 'acc_norm_stderr': 0.02189352994166582}, 'boolq': {'acc': 0.6226299694189602, 'acc_stderr': 0.008477957863309989}, 'mean': np.float64(0.4939320375665395)}
2025-07-22 21:10:57,003 - evaluate_grasp - INFO - 

===== mean acc: 0.4939320375665395 =====


2025-07-22 21:11:01,518 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 21:11:16,379 - __main__ - INFO - Layers order: 24,25,26,27,28,29,30,31,21,20,19,22,18,17,16,15,8,14,10,7,9,11,13,6,12,5,4,3,2,23,1,0
2025-07-22 21:11:16,379 - __main__ - INFO - Num prune: 10
2025-07-22 21:11:16,380 - __main__ - INFO - Layers to remove: [24, 25, 26, 27, 28, 29, 30, 31, 21, 20]
2025-07-22 21:11:16,380 - __main__ - INFO - ====================================================================================================
2025-07-22 21:11:16,381 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 21:14:59,446 - evaluate_grasp - INFO - wikitext2 3185.9853515625
2025-07-22 21:14:59,446 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 21:16:05,307 - evaluate_grasp - INFO - ptb 4515.50244140625
2025-07-22 23:40:06,573 - evaluate_grasp - INFO - {'wikitext2': 3185.9853515625, 'ptb': 4515.50244140625, 'mathqa': {'acc': 0.3078726968174204, 'acc_stderr': 0.008450437021151815, 'acc_norm': 0.29447236180904524, 'acc_norm_stderr': 0.00834410722096106}, 'piqa': {'acc': 0.6741022850924918, 'acc_stderr': 0.010935760218903946, 'acc_norm': 0.6637649619151251, 'acc_norm_stderr': 0.011022346708970236}, 'hellaswag': {'acc': 0.37801234813782114, 'acc_stderr': 0.004838997427699744, 'acc_norm': 0.489344752041426, 'acc_norm_stderr': 0.004988648260010039}, 'winogrande': {'acc': np.float64(0.5769534333070244), 'acc_stderr': 0.013885055359056474}, 'arc_easy': {'acc': 0.5669191919191919, 'acc_stderr': 0.010167478013701778, 'acc_norm': 0.5526094276094277, 'acc_norm_stderr': 0.01020283238541565}, 'arc_challenge': {'acc': 0.3455631399317406, 'acc_stderr': 0.01389693846114568, 'acc_norm': 0.3848122866894198, 'acc_norm_stderr': 0.014218371065251104}, 'openbookqa': {'acc': 0.272, 'acc_stderr': 0.019920483209566072, 'acc_norm': 0.382, 'acc_norm_stderr': 0.021750820591250834}, 'boolq': {'acc': 0.6281345565749236, 'acc_stderr': 0.008453018007354025}, 'mean': np.float64(0.46869470647257677)}
2025-07-22 23:40:06,575 - evaluate_grasp - INFO - 

===== mean acc: 0.46869470647257677 =====


2025-07-22 23:40:10,992 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-22 23:40:30,541 - __main__ - INFO - Layers order: 24,25,26,27,28,29,30,31,21,20,19,22,18,17,16,15,8,14,10,7,9,11,13,6,12,5,4,3,2,23,1,0
2025-07-22 23:40:30,542 - __main__ - INFO - Num prune: 12
2025-07-22 23:40:30,543 - __main__ - INFO - Layers to remove: [24, 25, 26, 27, 28, 29, 30, 31, 21, 20, 19, 22]
2025-07-22 23:40:30,543 - __main__ - INFO - ====================================================================================================
2025-07-22 23:40:30,545 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-22 23:42:09,090 - evaluate_grasp - INFO - wikitext2 8630.9462890625
2025-07-22 23:42:09,090 - evaluate_grasp - INFO - load dataset ptb
2025-07-22 23:42:39,817 - evaluate_grasp - INFO - ptb 12492.0771484375
2025-07-23 01:18:48,775 - evaluate_grasp - INFO - {'wikitext2': 8630.9462890625, 'ptb': 12492.0771484375, 'mathqa': {'acc': 0.28743718592964823, 'acc_stderr': 0.008284830813404287, 'acc_norm': 0.2820770519262982, 'acc_norm_stderr': 0.008238030326915533}, 'piqa': {'acc': 0.6381936887921654, 'acc_stderr': 0.011211397313020371, 'acc_norm': 0.6273122959738846, 'acc_norm_stderr': 0.011281318332897744}, 'hellaswag': {'acc': 0.34027086237801235, 'acc_stderr': 0.004728318577835235, 'acc_norm': 0.44692292372037445, 'acc_norm_stderr': 0.00496158757427562}, 'winogrande': {'acc': np.float64(0.6029992107340174), 'acc_stderr': 0.013751092519806704}, 'arc_easy': {'acc': 0.5143097643097643, 'acc_stderr': 0.010255580881603629, 'acc_norm': 0.48653198653198654, 'acc_norm_stderr': 0.010256060854840756}, 'arc_challenge': {'acc': 0.31313993174061433, 'acc_stderr': 0.013552671543623503, 'acc_norm': 0.34812286689419797, 'acc_norm_stderr': 0.013921008595179347}, 'openbookqa': {'acc': 0.248, 'acc_stderr': 0.019332342821239103, 'acc_norm': 0.342, 'acc_norm_stderr': 0.021236147199899268}, 'boolq': {'acc': 0.6287461773700306, 'acc_stderr': 0.008450174658715903}, 'mean': np.float64(0.44663710265678164)}
2025-07-23 01:18:48,776 - evaluate_grasp - INFO - 

===== mean acc: 0.44663710265678164 =====


2025-07-23 01:18:53,280 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-23 01:19:07,136 - __main__ - INFO - Layers order: 24,25,26,27,28,29,30,31,21,20,19,22,18,17,16,15,8,14,10,7,9,11,13,6,12,5,4,3,2,23,1,0
2025-07-23 01:19:07,136 - __main__ - INFO - Num prune: 14
2025-07-23 01:19:07,137 - __main__ - INFO - Layers to remove: [24, 25, 26, 27, 28, 29, 30, 31, 21, 20, 19, 22, 18, 17]
2025-07-23 01:19:07,137 - __main__ - INFO - ====================================================================================================
2025-07-23 01:19:07,138 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-23 01:20:44,815 - evaluate_grasp - INFO - wikitext2 23870.453125
2025-07-23 01:20:44,815 - evaluate_grasp - INFO - load dataset ptb
2025-07-23 01:21:17,232 - evaluate_grasp - INFO - ptb 33667.67578125
2025-07-23 02:51:28,122 - evaluate_grasp - INFO - {'wikitext2': 23870.453125, 'ptb': 33667.67578125, 'mathqa': {'acc': 0.24656616415410385, 'acc_stderr': 0.007890234123285118, 'acc_norm': 0.24690117252931323, 'acc_norm_stderr': 0.007893836965752419}, 'piqa': {'acc': 0.6327529923830251, 'acc_stderr': 0.011247128539690562, 'acc_norm': 0.6066376496191512, 'acc_norm_stderr': 0.011397418546689136}, 'hellaswag': {'acc': 0.3043218482374029, 'acc_stderr': 0.004591792612775612, 'acc_norm': 0.385381398127863, 'acc_norm_stderr': 0.004856906473719381}, 'winogrande': {'acc': np.float64(0.5493291239147593), 'acc_stderr': 0.01398392886904024}, 'arc_easy': {'acc': 0.4238215488215488, 'acc_stderr': 0.010140006095213606, 'acc_norm': 0.38846801346801346, 'acc_norm_stderr': 0.01000127604448523}, 'arc_challenge': {'acc': 0.28242320819112626, 'acc_stderr': 0.013155456884097222, 'acc_norm': 0.3174061433447099, 'acc_norm_stderr': 0.01360223908803817}, 'openbookqa': {'acc': 0.234, 'acc_stderr': 0.018952741564893683, 'acc_norm': 0.336, 'acc_norm_stderr': 0.021144791425048846}, 'boolq': {'acc': 0.6244648318042814, 'acc_stderr': 0.008469774334938068}, 'mean': np.float64(0.41220996468828097)}
2025-07-23 02:51:28,123 - evaluate_grasp - INFO - 

===== mean acc: 0.41220996468828097 =====


2025-07-23 02:51:32,248 - __main__ - INFO - Loading model: meta-llama/Llama-3.1-8B
2025-07-23 02:51:41,699 - __main__ - INFO - Layers order: 24,25,26,27,28,29,30,31,21,20,19,22,18,17,16,15,8,14,10,7,9,11,13,6,12,5,4,3,2,23,1,0
2025-07-23 02:51:41,699 - __main__ - INFO - Num prune: 16
2025-07-23 02:51:41,700 - __main__ - INFO - Layers to remove: [24, 25, 26, 27, 28, 29, 30, 31, 21, 20, 19, 22, 18, 17, 16, 15]
2025-07-23 02:51:41,700 - __main__ - INFO - ====================================================================================================
2025-07-23 02:51:41,701 - evaluate_grasp - INFO - load dataset wikitext2
2025-07-23 02:53:09,194 - evaluate_grasp - INFO - wikitext2 97390.1015625
2025-07-23 02:53:09,194 - evaluate_grasp - INFO - load dataset ptb
2025-07-23 02:53:33,801 - evaluate_grasp - INFO - ptb 96339.40625
2025-07-23 04:13:55,186 - evaluate_grasp - INFO - {'wikitext2': 97390.1015625, 'ptb': 96339.40625, 'mathqa': {'acc': 0.2184254606365159, 'acc_stderr': 0.007563754466495908, 'acc_norm': 0.22948073701842547, 'acc_norm_stderr': 0.007697779360944249}, 'piqa': {'acc': 0.5990206746463548, 'acc_stderr': 0.011434766962108305, 'acc_norm': 0.5674646354733406, 'acc_norm_stderr': 0.011559142916063147}, 'hellaswag': {'acc': 0.2869946225851424, 'acc_stderr': 0.004514345547780338, 'acc_norm': 0.35301732722565227, 'acc_norm_stderr': 0.004769313300470234}, 'winogrande': {'acc': np.float64(0.5501183898973955), 'acc_stderr': 0.013981711904049735}, 'arc_easy': {'acc': 0.3164983164983165, 'acc_stderr': 0.009543851857323888, 'acc_norm': 0.31523569023569026, 'acc_norm_stderr': 0.009533589368505858}, 'arc_challenge': {'acc': 0.2687713310580205, 'acc_stderr': 0.012955065963710682, 'acc_norm': 0.31313993174061433, 'acc_norm_stderr': 0.01355267154362351}, 'openbookqa': {'acc': 0.218, 'acc_stderr': 0.018483378223178856, 'acc_norm': 0.316, 'acc_norm_stderr': 0.020812359515855867}, 'boolq': {'acc': 0.6238532110091743, 'acc_stderr': 0.008472516562330725}, 'mean': np.float64(0.38521025079136506)}
2025-07-23 04:13:55,188 - evaluate_grasp - INFO - 

===== mean acc: 0.38521025079136506 =====


